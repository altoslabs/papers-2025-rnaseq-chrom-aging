{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import copy\n",
    "from sklearn.cluster import SpectralClustering, DBSCAN\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.sparse.linalg import svds\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load configuration file"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### Load configuration file\n",
    "with open('params_preprocess.yaml', 'rb') as f:\n",
    "    conf = yaml.safe_load(f.read())\n",
    "\n",
    "fsz = 28\n",
    "# Load general settings from the config file\n",
    "save_img = conf['settings']['save_img']\n",
    "active_dataset = conf[\"settings\"][\"active_dataset\"]\n",
    "dataset_config = conf[\"datasets\"][active_dataset]\n",
    "\n",
    "# Load dataset-specific parameters\n",
    "input_file = dataset_config[\"input_file\"]\n",
    "output_folder = dataset_config[\"output_folder\"]\n",
    "gamma_value = dataset_config['gamma_value']\n",
    "rep_list = dataset_config[\"rep_list\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def data_preprocessing(df,expt_list):\n",
    "    normalized_df = df[['Gene_ID','StartPos','EndPos']]\n",
    "    for expt in expt_list:\n",
    "        normalized_df[expt] = (df[expt])/  df[expt].sum()\n",
    "    normalized_df = normalized_df.reset_index()\n",
    "    return  normalized_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def save_cluster(df, save_plot_path):\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(12, 10)\n",
    "    fig.set_dpi(150)\n",
    "    df['index'] = range(1, len(df)+1)\n",
    "    df['log_index'] = np.log10(df['index'])\n",
    "    palette = [ \"#008000\",\"#FF0000\"]\n",
    "    for i in df.columns[0:3]:\n",
    "        sns.scatterplot(ax=ax, x=\"index\", y=i, data=df, hue=\"labels\",s=fsz+50,palette=sns.color_palette(palette, 3),linewidth=0, alpha = 0.7)\n",
    "\n",
    "    ax.set_ylabel(r'$\\alpha_t$', fontsize = fsz, color='k')\n",
    "    ax.set_xlabel('gene expression rank ($\\mathrm{r}_t$)', fontsize=fsz, color='k')\n",
    "    ax.tick_params('both',which='major', length=7,labelsize=fsz)\n",
    "    ax.tick_params('both',which='minor', length=7,labelsize=fsz)\n",
    "    ax.set(xscale= 'log')\n",
    "    ax.set(yscale= 'log')\n",
    "    labels = ['High abundance transcript (HAT)','Low abundance transcript (LAT)']\n",
    "    legend_elements = [Line2D([0], [0], marker='o',color = 'r', lw=0, markerfacecolor='r', label=labels[0],\n",
    "                                  markersize=8),\n",
    "                        Line2D([0], [0], marker='o',color = 'g', lw=0, markerfacecolor='g', label=labels[1],\n",
    "                                  markersize=8)]\n",
    "\n",
    "    ax.legend(handles=legend_elements,markerscale=2, loc='upper right', borderaxespad=0.05, fontsize = fsz-4)\n",
    "    plt.savefig(save_plot_path)\n",
    "    plt.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def cluster(df, expt_list):\n",
    "    X = df.drop(['StartPos','EndPos','Gene_ID'], axis=1)\n",
    "    df['labels'] = SpectralClustering(n_neighbors=100, assign_labels='discretize',\n",
    "                                      random_state=123, gamma=gamma_value, n_clusters=2,\n",
    "                                      affinity=\"laplacian\", eigen_solver=\"arpack\").fit_predict(X)\n",
    "    if save_img:\n",
    "        save_cluster(copy.deepcopy(df.apply(lambda x: x.sort_values(ascending=False).values)), os.path.join(chromosome_output_folder, expt_list[0]+\"_LAT_HAT_cluster.png\"))\n",
    "\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def high_lo_preprocessing(df):\n",
    "    X = df.drop(['StartPos','EndPos','Gene_ID'], axis=1)\n",
    "    df['labels'] = DBSCAN(metric=\"manhattan\").fit(X).labels_\n",
    "    reg_df = df[df.labels != -1]\n",
    "    abnormal_df = df[df.labels == -1]\n",
    "    return reg_df, abnormal_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_high_lo(df, expt_list):\n",
    "    df = cluster(df, expt_list)\n",
    "    df['mean_value'] = df[expt_list].mean(axis=1)\n",
    "    temp = df.groupby('labels')['mean_value'].mean().rename(expt_list[0]).reset_index()\n",
    "    hi_cluster_label = temp[temp[expt_list[0]] == temp[expt_list[0]].max()]['labels'].reset_index(drop=True)[0]\n",
    "    lo_cluster_label = temp[temp[expt_list[0]] == temp[expt_list[0]].min()]['labels'].reset_index(drop=True)[0]\n",
    "    df_hi = df[df.labels == hi_cluster_label]\n",
    "    df_lo = df[df.labels == lo_cluster_label]\n",
    "    print(\"Bucket sizes (HAT, LAT):\", len(df_hi), len(df_lo))\n",
    "    return df_hi, df_lo, hi_cluster_label, lo_cluster_label"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA and SVD computation functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def extract_features_SVD(df,k):\n",
    "    U, sigma, Vt = svds(df.to_numpy(),k=k)\n",
    "    wv = pd.DataFrame(U*sigma) #N X k\n",
    "    return wv"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def extract_features_PCA(df):\n",
    "    pca = PCA(n_components=10)\n",
    "    principalComponents = pca.fit_transform(df)\n",
    "    principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1','principal component 2','principal component 3','principal component 4','principal component 5','principal component 6','principal component 7','principal component 8','principal component 9','principal component 10'])\n",
    "    return principalDf"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def Compute_PCA_SVD(normalized_df, df_PCA, df_SVD, indices_names, count, k):\n",
    "    red_normalized_df = normalized_df.drop(columns=['StartPos','EndPos','index'])\n",
    "    df_t = red_normalized_df.set_index('Gene_ID').T.sort_index(ascending=True)\n",
    "    df_t.columns = df_t.columns.astype(str)\n",
    "\n",
    "    df_t_scaled = pd.DataFrame(df_t)\n",
    "    \n",
    "    principal_component = extract_features_PCA(df_t_scaled)\n",
    "    principal_component = principal_component.set_index(df_t.index)\n",
    "    principal_component.columns = principal_component.columns.astype(str)\n",
    "    \n",
    "    wv = extract_features_SVD(df_t,k)\n",
    "    wv = wv.set_index(df_t.index)\n",
    "    wv_scaled = StandardScaler().fit_transform(wv.values)\n",
    "    wv_scaled = pd.DataFrame(wv_scaled, index=wv.index, columns=wv.columns)\n",
    "    \n",
    "    if count == 0:\n",
    "        df_PCA = pd.DataFrame(cdist(principal_component, principal_component, 'cityblock'))\n",
    "        df_SVD = pd.DataFrame(cdist(wv_scaled, wv_scaled, 'cityblock'))\n",
    "        count = count+1\n",
    "        indices_names = list(principal_component.index)\n",
    "        \n",
    "    else:\n",
    "        temp_val = pd.DataFrame(cdist(principal_component, principal_component, 'cityblock'))\n",
    "        df_PCA = df_PCA+temp_val\n",
    "        temp_val = pd.DataFrame(cdist(wv_scaled, wv_scaled, 'cityblock'))\n",
    "        df_SVD = df_SVD+temp_val\n",
    "        count = count+1\n",
    "    \n",
    "    return df_PCA, df_SVD, indices_names, count"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main processing pipeline to compute and store HAT/LAT data per chromosome"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "df = pd.read_csv(input_file)\n",
    "df['Chromosome'] = df['Chromosome'].astype(str) ##\n",
    "df.sort_values(\"Gene_ID\", inplace=True)\n",
    "df_PCA = pd.DataFrame()\n",
    "df_SVD = pd.DataFrame()\n",
    "indices_names=[]\n",
    "k = 20 # SVD size\n",
    "count = 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "## Flattening rep_list (list of lists) into a single list \n",
    "expt_list_flat = [item for sublist in rep_list for item in sublist] "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "for chromosome, data in df.groupby('Chromosome'):\n",
    "    if chromosome.isalnum() and chromosome not in ['MT', 'Y']:\n",
    "        print(\"Processing chromosome:\", chromosome)\n",
    "        \n",
    "        chromosome_output_folder = os.path.join(output_folder, \"chromosome_\" + str(chromosome))\n",
    "        os.makedirs(chromosome_output_folder, exist_ok=True)\n",
    "        data_hi_folder = os.path.join(chromosome_output_folder, \"data_hi\")\n",
    "        data_lo_folder = os.path.join(chromosome_output_folder, \"data_lo\")\n",
    "        os.makedirs(data_hi_folder, exist_ok=True)\n",
    "        os.makedirs(data_lo_folder, exist_ok=True)\n",
    "\n",
    "        normalized_df = data_preprocessing(data, expt_list_flat)\n",
    "        # Calculate PCA of the RNAseq data\n",
    "        df_PCA, df_SVD, indices_names, count = Compute_PCA_SVD(normalized_df, df_PCA, df_SVD, indices_names, count, k)\n",
    "\n",
    "        counter = 0\n",
    "        new_df = None\n",
    "\n",
    "        for rep in rep_list:\n",
    "            print(\"Processing replicate group:\", rep)\n",
    "            subset = copy.deepcopy(rep) + ['StartPos', 'EndPos', 'Gene_ID']\n",
    "            subset_df = normalized_df[subset]\n",
    "            reg_df, abnormal_df = high_lo_preprocessing(subset_df)\n",
    "            data_hi, data_lo, hi_cluster_label, lo_cluster_label = get_high_lo(reg_df, rep)\n",
    "            data_hi_file = os.path.join(data_hi_folder, f\"chromosome_{chromosome}_rep_{rep[0]}_data_hi.csv\")\n",
    "            data_lo_file = os.path.join(data_lo_folder, f\"chromosome_{chromosome}_rep_{rep[0]}_data_lo.csv\")\n",
    "            data_hi.to_csv(data_hi_file)\n",
    "            data_lo.to_csv(data_lo_file)\n",
    "            \n",
    "            if counter == 0:\n",
    "                new_df = pd.concat([data_hi, data_lo], axis=0)[['StartPos', 'EndPos', 'Gene_ID', 'labels'] + rep].reset_index(drop=True)\n",
    "            else:  \n",
    "                merge_df = pd.concat([data_hi, data_lo], axis=0)[['Gene_ID', 'labels'] + rep].reset_index(drop=True)\n",
    "                new_df = pd.merge(new_df, merge_df, how='outer', on='Gene_ID')\n",
    "            new_df.sort_values(by=['StartPos', 'EndPos'], inplace=True)\n",
    "            counter += 1\n",
    "            \n",
    "            for exp in rep:\n",
    "                new_df[exp] = new_df['labels']  \n",
    "            new_df.drop(['labels'], axis=1, inplace=True)\n",
    "\n",
    "        out_file = os.path.join(chromosome_output_folder, f\"chromosome_{chromosome}_ec_hc_mask_xy.csv\")\n",
    "        new_df.to_csv(out_file, index=False)\n",
    "        print(f\"Saved HAT/LAT mask data for chromosome {chromosome} in {out_file}\")\n",
    "print(\"HAT and LAT gene computations completed and stored per chromosome.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and PCA and SVD data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Saving PCA of transcriptomic data.\n",
    "df_PCA = df_PCA/count\n",
    "df_PCA['experiments']=indices_names\n",
    "df_PCA = df_PCA.set_index('experiments').T\n",
    "df_PCA = df_PCA.rename_axis(\"chromosome_id\").reset_index()\n",
    "df_PCA.to_csv(os.path.join(output_folder,\"fpkm_data_PCA.csv\"),index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Saving SVD of transcriptomic data.\n",
    "df_SVD = df_SVD/count\n",
    "df_SVD['experiments']=indices_names\n",
    "df_SVD = df_SVD.set_index('experiments').T\n",
    "df_SVD = df_SVD.rename_axis(\"chromosome_id\").reset_index()\n",
    "df_SVD.to_csv(os.path.join(output_folder,\"fpkm_data_SVD20.csv\"),index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
